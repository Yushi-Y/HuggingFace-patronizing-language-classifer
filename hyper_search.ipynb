{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "from sklearn.model_selection import KFold\n",
    "from models.tuner import *\n",
    "from models.model_pool import *\n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from models.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_once(\n",
    "    pretrained_model_name,\n",
    "    classifier_dim,\n",
    "    lr,\n",
    "    categorical,\n",
    "    tokenizer_max_len,\n",
    "    gradient_clip_val,\n",
    "    target_label,\n",
    "    batch_size,\n",
    "    dropout,\n",
    "    num_classes,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    tuner = ModelTuner(target_label)\n",
    "    full_df, test_df = tuner.load_data()\n",
    "\n",
    "    score_ls = []\n",
    "\n",
    "    for train_idx, val_idx in KFold(3, shuffle=True).split(full_df):\n",
    "        train_df = full_df.iloc[train_idx]\n",
    "        train_df = tuner.downsample_data(train_df)\n",
    "        val_df = full_df.iloc[val_idx]\n",
    "        model = TransformerModel(\n",
    "            pretrained_model_name,\n",
    "            add_categorical=categorical,\n",
    "            num_classes=num_classes,\n",
    "            lr=lr,\n",
    "            classifier_dim=classifier_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        trainer, metrics = tuner.train_once(\n",
    "            train_df,\n",
    "            epochs=epochs,\n",
    "            patience=patience,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            batch_size=batch_size,\n",
    "            gradient_clip_val=gradient_clip_val,\n",
    "            tokenizer_max_len=tokenizer_max_len,\n",
    "            val_df=val_df,\n",
    "        )\n",
    "        score_ls.append(metrics[\"f1\"])\n",
    "        \n",
    "        del model, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return score_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "patience = 2\n",
    "\n",
    "# ordered by performance impact\n",
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768], ['distilbert-base-cased', 768], ['roberta-base', 768]],\n",
    "    'tokenizer_max_len': [16, 32, 64, 128, 256],\n",
    "    'lr': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [32, 16, 8],\n",
    "    'target_label': [['label', 2], ['orig_label', 5]],\n",
    "    'categorical': [True, False],\n",
    "    'dropout': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: ['distilbert-base-uncased', 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for pretrained_model_name, classifier_dim in hparams[\"model_arch\"]:\n",
    "    # pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": hparams['lr'][0],\n",
    "        \"categorical\": hparams['categorical'][0],\n",
    "        \"tokenizer_max_len\": hparams['tokenizer_max_len'][0],\n",
    "        \"gradient_clip_val\": hparams['gradient_clip_val'][0],\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": hparams['batch_size'][0],\n",
    "        \"dropout\": hparams['dropout'][0],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best tokenizer max length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [16, 32, 64, 128, 256],\n",
    "    'lr': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [32, 16, 8],\n",
    "    'target_label': [['label', 2], ['orig_label', 5]],\n",
    "    'categorical': [True, False],\n",
    "    'dropout': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for tokenizer_max_len in hparams[\"tokenizer_max_len\"]:\n",
    "    pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": hparams['lr'][0],\n",
    "        \"categorical\": hparams['categorical'][0],\n",
    "        \"tokenizer_max_len\": tokenizer_max_len,\n",
    "        \"gradient_clip_val\": hparams['gradient_clip_val'][0],\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": hparams['batch_size'][0],\n",
    "        \"dropout\": hparams['dropout'][0],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [128],\n",
    "    'lr': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [32, 16, 8],\n",
    "    'target_label': [['label', 2], ['orig_label', 5]],\n",
    "    'categorical': [True, False],\n",
    "    'dropout': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for lr in hparams[\"lr\"]:\n",
    "    pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": lr,\n",
    "        \"categorical\": hparams['categorical'][0],\n",
    "        \"tokenizer_max_len\": hparams['tokenizer_max_len'][0],\n",
    "        \"gradient_clip_val\": hparams['gradient_clip_val'][0],\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": hparams['batch_size'][0],\n",
    "        \"dropout\": hparams['dropout'][0],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [128],\n",
    "    'lr': [1e-4],\n",
    "    'batch_size': [32, 16, 8],\n",
    "    'target_label': [['label', 2], ['orig_label', 5]],\n",
    "    'categorical': [True, False],\n",
    "    'dropout': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for batch_size in hparams[\"batch_size\"]:\n",
    "    pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": hparams['lr'][0],\n",
    "        \"categorical\": hparams['categorical'][0],\n",
    "        \"tokenizer_max_len\": hparams['tokenizer_max_len'][0],\n",
    "        \"gradient_clip_val\": hparams['gradient_clip_val'][0],\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"dropout\": hparams['dropout'][0],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best label strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: use binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [128],\n",
    "    'lr': [1e-4],\n",
    "    'batch_size': [32],\n",
    "    'target_label': [['label', 2], ['orig_label', 5]],\n",
    "    'categorical': [True, False],\n",
    "    'dropout': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for target_label, num_classes in hparams[\"target_label\"]:\n",
    "    pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    # target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": hparams['lr'][0],\n",
    "        \"categorical\": hparams['categorical'][0],\n",
    "        \"tokenizer_max_len\": hparams['tokenizer_max_len'][0],\n",
    "        \"gradient_clip_val\": hparams['gradient_clip_val'][0],\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": hparams['batch_size'][0],\n",
    "        \"dropout\": hparams['dropout'][0],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search if add categorical feature is useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: not use categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [128],\n",
    "    'lr': [1e-4],\n",
    "    'batch_size': [32],\n",
    "    'target_label': [['label', 2]],\n",
    "    'categorical': [True, False],\n",
    "    'dropout': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for categorical in hparams[\"categorical\"]:\n",
    "    pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": hparams['lr'][0],\n",
    "        \"categorical\": categorical,\n",
    "        \"tokenizer_max_len\": hparams['tokenizer_max_len'][0],\n",
    "        \"gradient_clip_val\": hparams['gradient_clip_val'][0],\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": hparams['batch_size'][0],\n",
    "        \"dropout\": hparams['dropout'][0],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [128],\n",
    "    'lr': [1e-4],\n",
    "    'batch_size': [32],\n",
    "    'target_label': [['label', 2]],\n",
    "    'categorical': [False],\n",
    "    'dropout': [0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for dropout in hparams[\"dropout\"]:\n",
    "    pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": hparams['lr'][0],\n",
    "        \"categorical\": hparams['categorical'][0],\n",
    "        \"tokenizer_max_len\": hparams['tokenizer_max_len'][0],\n",
    "        \"gradient_clip_val\": hparams['gradient_clip_val'][0],\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": hparams['batch_size'][0],\n",
    "        \"dropout\": dropout,\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best gradient clip value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [128],\n",
    "    'lr': [1e-4],\n",
    "    'batch_size': [32],\n",
    "    'target_label': [['label', 2]],\n",
    "    'categorical': [False],\n",
    "    'dropout': [0.3],\n",
    "    'gradient_clip_val': [0.1, 1, 10, 100, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for gradient_clip_val in hparams[\"gradient_clip_val\"]:\n",
    "    pretrained_model_name, classifier_dim = hparams[\"model_arch\"][0]\n",
    "    target_label, num_classes = hparams['target_label'][0]\n",
    "    config = {\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"classifier_dim\": classifier_dim,\n",
    "        \"lr\": hparams['lr'][0],\n",
    "        \"categorical\": hparams['categorical'][0],\n",
    "        \"tokenizer_max_len\": hparams['tokenizer_max_len'][0],\n",
    "        \"gradient_clip_val\": gradient_clip_val,\n",
    "        \"target_label\": target_label,\n",
    "        \"batch_size\": hparams['batch_size'][0],\n",
    "        \"dropout\": hparams['dropout'][0],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    score_ls = cross_validate_once(**config)\n",
    "    with open('parameter_search.txt', 'a') as f:\n",
    "        f.write(f'{np.mean(score_ls)} {str(score_ls)} {str(config)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'model_arch': [['distilbert-base-uncased', 768]],\n",
    "    'tokenizer_max_len': [128],\n",
    "    'lr': [1e-4],\n",
    "    'batch_size': [32],\n",
    "    'target_label': [['label', 2]],\n",
    "    'categorical': [False],\n",
    "    'dropout': [0.3],\n",
    "    'gradient_clip_val': [100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27f2fe8eb6dccfd7b08bd09c35139f2f1917d14e9573d68481e9d3ed4ea63315"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
